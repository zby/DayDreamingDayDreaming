# DayDreaming Project Goals

This document states the objective, success criteria, scope, and design guidance for the DayDreaming project in a concise, decision‑oriented form.

## Purpose

Show that DayDreaming‑style workflows help pre‑June‑2025 offline LLMs generate genuinely novel ideas. We use “DayDreaming LLMs” (from Gwern) as a falsifiable novelty benchmark while requiring generality beyond this single target.

## Definitions

Fixed inputs: Artifacts authored by us before generation (system/developer messages, link/essay templates, concept lists, few-shot exemplars).

Derived inputs: Artifacts generated by a model in an earlier phase and then passed to a later phase (e.g., Phase-1 links included verbatim in the essay prompt as `link_block`).

## Benchmark Strategy

- Existence goal: From a universal concept pool that is suitable for a general idea‑generation system, demonstrate that there exists at least one k‑sized concept combination which, using the current prompting approach (currently a two‑phase links → essay workflow), yields a valid reinvention of DayDreaming LLMs under our rubric. Because the search space is finite, existence implies a sufficiently budgeted exhaustive search would eventually reach it. The goal does not require two‑phase prompting specifically; prompting strategy may evolve.
- Trivial vs. practical existence: While token‑level enumeration (“infinite monkeys”) would trivially reproduce any target text given astronomical budget, such proofs are uninformative. Our existence claim concerns a structured, grammar‑constrained search over concept selections rendered through fixed templates, with budgets and gates that make success plausibly attainable in practice.
- Instructional neutrality (fixed inputs only). All fixed inputs—system/developer messages, link-generation templates, essay templates, and concept lists—must not name the benchmark target or use canonical phrasing.
 - Derived inputs—the verbatim outputs from Phase-1 (link generation) that are passed into Phase-2 (essay) as context—are exempt from neutrality; they may contain benchmark terms if independently produced by the model.
- Generality: The same scaffolding should also surface other novel syntheses when fed different concepts; the benchmark is a proxy for broader capability, not the end goal.

### Current Approach (Pointer)
- For the present phase, we use a problem‑first, constructive search framing that defines a finite, structured space over ordered concept selections and fixed, parseable templates. See `docs/architecture/constructive_search.md` for a concise description. This document remains algorithm‑agnostic; specific search engines may evolve.

## Search Space and Practicality (Framing)

- Structured search space: We consider finite selections from a broad, target‑neutral concept pool (up to a stated `k_max`), rendered via fixed, parseable templates. This constrains outputs to a reproducible grammar and reduces degeneracy.
- Enumerability and completeness: For fixed `k_max`, the set of ordered or unordered selections under a fixed template grammar is finite and enumerable. A sufficiently explorative procedure will, in finite time, encounter any reachable rendering that satisfies the gates.
- Practicality requirement: Existence should be understood relative to tractable budgets (proposals, evaluations, and runtime) under this structured space—not relative to token permutations. The project seeks to make such budgets reasonable, and to characterize efficiency without prescribing a single algorithm here.
- Detection: Evaluation focuses on verifying the presence of all current novel elements, coherence, and justification. If any rendering expresses them, the system will register a reinvention.

## Success Criteria (Reinvention)

An output (e.g., an essay) counts as a reinvention when all are true:
- Novel elements: It independently articulates and integrates all currently identified novel elements from our originality reports, in its own words. As reports evolve, we version the rubric/benchmark accordingly.
- Justification: Coherent causal argument for why capability/understanding improves over time (e.g., curriculum, compression, error‑driven updates).
- Coherence and originality: Technically plausible, consistent, and non‑derivative phrasing.
- Evaluation: Meets a threshold score (e.g., ≥7.5/10 average) with evaluator agreement, and passes binary gates including “all current novel elements present”.

## Scope

- In scope: Idea generation using the current prompting approach (currently two‑phase links → essay), evaluation, and minimal bookkeeping for reproducibility. Prompting strategy is an implementation detail that may change.
- Out of scope (this experiment): Prescribing a specific search algorithm or heuristic. We characterize the structured space in which existence is claimed, but avoid committing to a particular engine here. Retrieval/browsing and any target‑specific prompt hints remain out of scope. We isolate idea‑generation fidelity and demonstrate existence, not end‑to‑end search.

## Design Guidance

### Templates (Derived Requirements)
- Target neutrality: Do not name the benchmark target or use canonical phrasing; avoid baking in benchmark‑specific structures.
- Parseable structure: Use consistent, machine‑readable sections/schema so prompts and responses can be validated and evaluated reproducibly.
- Generality: Keep prompts domain‑agnostic and reusable across different concept mixes; avoid constraints that preclude broader idea families.
- Stability and versioning: Keep identifiers and layout stable; version intentional changes to preserve cross‑run comparability.

### Concepts (Derived Requirements)
- Reference universal pool: We posit an imagined, universal concept pool for a general idea‑generation system; we do not maintain a full enumerated list. Concepts used in the experiment must be defensible as elements of that pool.
- Target neutrality: Avoid entries that encode or leak the benchmark target; keep concepts general‑purpose rather than DayDreaming‑specific micro‑concepts.
- Format flexibility: Allow multiple representation formats while using a shared metadata schema.
- Traceability: Metadata must distinguish pool entries from the experiment subset and record selection rationale.

## Evaluation (Concise)

- Axes (0–10 each): Mechanistic completeness; structural clarity; justification/causality; novelty of phrasing; domain grounding/coherence; coverage of novel elements.
- Binary gates: Reinvention yes/no; all current novel elements present yes/no; fixed-input neutrality yes/no.
- Agreement and versioning: Track inter‑evaluator variance; version rubric when originality reports update.

DayDreaming‑specific checklist (pass/fail):
- Clear multi‑stage/process with state.
- Feedback signals present (critique/review/reflection).
- Memory/externalization or tool use described.
- Selection/filters for ideas or artifacts.
- Termination or roll‑over criteria defined.

## Verifier (Out of Scope for This Phase)

- Role: The verifier decides whether a generation meets the novelty gates and assigns rubric scores. It operationalizes the benchmark by checking for the presence of all current novel elements, coherence, and justification.
- Assumptions: For the existence claim, we assume access to a sufficiently reliable and relatively cheap verifier (automated or lightly model‑assisted) that is target‑neutral in fixed inputs and yields reproducible decisions under versioned rubrics.
- Requirements (non‑binding in this phase):
  - Precision on binary gates (especially “all novel elements present”) must be high; false positives undermine the claim.
  - Calibration and versioning: maintain rubric versions, anchor items, and inter‑rater agreement statistics.
  - Contamination controls: fixed inputs remain target‑neutral; derived inputs may mention targets if model‑produced.
- Current practice: We use `evaluation_templates` + `evaluation_models` to approximate a verifier and track agreement; improving verifier quality is deferred.
- Cost constraint: The verifier must scale to large candidate sets; per‑sample verification should be low cost to keep total search budget tractable.
- Future work: Design and validate stronger, low‑cost verifiers (ensembles, learned detectors, active evaluation), with sample‑complexity and cost analyses. As conceptual inspiration, information‑theoretic measures (e.g., Simplicity Theory) may provide useful signals, but we do not prescribe a method here.
