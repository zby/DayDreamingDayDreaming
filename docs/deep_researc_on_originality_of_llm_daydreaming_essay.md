
# Analyzing the Originality of Ideas in *“AI Daydreaming”* (Gwern)

## Introduction

Gwern’s *“AI Daydreaming”* (titled *“LLM Daydreaming”* on gwern.net) proposes that large language models (LLMs) are missing key human-like faculties – namely the ability to learn continuously and to *“daydream”* or engage in background, self-directed thought. The essay outlines a **daydreaming loop (DDL)** algorithm as a potential solution and discusses its costs (*“daydreaming tax”*) and strategic implications. This analysis breaks down the article’s main ideas, compares them with earlier sources (papers, blogs, forums) for similar concepts, and evaluates which ideas are novel versus previously discussed. We then summarize each idea’s originality in a table and estimate what fraction of the article’s content is truly new.

## Main Ideas in *“AI Daydreaming”* and Their Precedents

### 1. **Missing Capability:** Continual Learning in LLMs

**Gwern’s Claim:** Current LLMs are *“frozen”* after training, unable to learn from new experience or feedback. This is likened to an *anterograde amnesiac* who cannot form new memories. Gwern suggests this lack of on-the-fly learning may explain why LLMs haven’t produced groundbreaking insights – they cannot transcend their static training knowledge.

**Precedents:** The limitation of LLMs being static is well-recognized. AI researchers have long explored **continual/lifelong learning** and *online* model adaptation. For example, *dynamic evaluation* (Krause et al. 2018) allows updating a language model on recent data at test time. This technique, among others, predates Gwern’s article by several years. More recently (June 2025), Dwarkesh Patel highlighted the *“huge problem”* that *“LLMs don’t get better over time the way a human would”* – there’s *“no way to give a model high level feedback”*, so you’re *“stuck with the abilities you get out of the box”*. In other words, it was already understood in the AI community that today’s models cannot learn incrementally through use. Gwern himself notes this is a *“long-standing”* issue. Thus, the idea that continual learning is missing in LLMs is **not original** – it’s a well-established point in AI literature and discussions.

**Originality:** **Low.** Identifying the lack of continual learning as a barrier is a widely discussed concept (e.g. in dynamic training research and AI forums) and is not a novel insight of the article.

### 2. **Missing Capability:** Default Mode “Daydreaming” (Continual Thinking)

**Gwern’s Claim:** Human creativity benefits from our *“default mode network”* (DMN) – the brain’s background activity during rest, which produces spontaneous ideas and insights. LLMs, by contrast, have *“no ‘default mode’ for background processing”*, meaning they never *mind-wander* or generate ideas without an explicit prompt. Gwern argues this absence of a “daydreaming” process could be another reason LLMs haven’t had eureka moments. Humans unconsciously form novel connections (e.g. sudden puns or solutions appearing *“while doing nothing”*), whereas an AI like ChatGPT sits idle until instructed.

**Precedents:** The notion that AI lacks a counterpart to the human default mode has indeed been raised before. For instance, an early 2025 article in *Medium* posed the question *“Your brain has a default mode. AI doesn’t. Should it?”*, noting that AI *“never wanders… never rests. They don’t dream or drift”* the way humans do. That piece underscores that AI has *“no spontaneous thought… no inner monologue… no mental itch to solve something just for fun”*. It concludes that this makes current AIs *“uncreative in a deeply human way”* – lacking the random, introspective incubation that often leads to insight.

Academic and blog discussions have begun to consider **simulated mind-wandering for AI**. Notably, researchers at the Allen Institute for AI (AI2) explored in 2023 whether an ML model could enter an *“off-task mode”* to generate *random associations* internally; they found it *“occasionally \[produced] more novel ideas.”*. This directly parallels Gwern’s suggestion and shows the concept was *already in the air* within the research community. Likewise, on AI forums, commenters have pointed out that *“LLMs lack any equivalent of a default mode network”*, meaning they do no thinking *“when not explicitly tasked,”* unlike humans who have constant background thoughts. Such remarks appeared in context of explaining LLMs’ limitations in creativity, even before Gwern’s essay.

In cognitive science and neuroscience, the link between the default mode network and creative insight (via daydreaming, incubation, etc.) was established well before; Gwern leverages this known science. The *application* of this concept to AI – i.e. asking *“should AI have downtime to daydream?”* – was emerging prior to July 2025 in thought pieces and early research (as above). However, it was not mainstream in AI development, and Gwern’s essay gave the idea a more concrete form.

**Originality:** **Moderate.** The general concept that *“AI never daydreams but maybe it should”* had been articulated by others in 2023–2025. Gwern’s contribution is reinforcing this idea and bringing clarity to it (framing it as a missing *faculty*). The idea itself is **not entirely new**, though – it builds on an existing analogy between human DMN and potential AI background processes, an analogy already noted in blogs and nascent research.

### 3. **Proposed Solution:** The *Day-Dreaming Loop (DDL)* Algorithm

**Gwern’s Proposal:** To address the above deficits, Gwern outlines a *Day-Dreaming Loop* – an algorithmic process meant to simulate a “default mode” for an AI. In the DDL, a generative model periodically retrieves *two random* memory items (facts or concepts) and *“thinks about them”* by attempting to form a connection, hypothesis or analogy between the pair. A separate *critic* model then evaluates the result to decide if the idea is *“interesting”* (novel and valuable). Any *hits* – non-obvious connections deemed worthwhile – are fed back into the system’s knowledge base (stored or even used to update the model). This way, new ideas become seeds for further creative combinations, creating a self-reinforcing loop of knowledge discovery. Essentially, the AI “daydreams” in the background, performing *combinatorial search* over its learned concepts to generate insights, much as a person might have flashes of insight during idle moments.

**Precedents:** While no prior work lays out this exact loop as a packaged algorithm for LLMs, **many elements of it have known precedents**:

* *Combinatorial Creativity (Random Concept Association):* The core of DDL – picking unrelated concepts and combining them – echoes long-established theories of creativity. Psychologists and authors (going back to Arthur Koestler in 1964) argued that novel ideas often arise from *“bisociation”* – the intersection of two disparate concepts. This is not a new insight. In fact, recent AI creativity techniques explicitly leverage this: e.g. a 2023 Harvard Business Review article introduced *“trisociation”* (using **three** random concepts with GPT-3/4 to spur innovation). By early 2025, companies were already using LLMs to facilitate such combinatorial brainstorming (Ipsos’s Four.AI workshops, etc.). Thus, the idea of an AI generating novel ideas by randomly pairing or grouping concepts **exists in prior art** – it’s a known creative strategy (humans have used random prompts for brainstorming, and AI-assisted ideation tools have begun to automate this). Gwern’s DDL essentially formalizes bisociation as a continuous background process.

* *Generator + Critic architecture:* Using a separate evaluator to filter the AI’s own generated outputs is a well-trodden idea in ML. For instance, generative adversarial networks (GANs) pit a generator against a discriminator (critic) to produce novel outputs, exploiting the fact that judging an output is easier than creating a good one. Gwern explicitly notes the *“generator–verifier gap”* – it’s *“easier to discriminate than to generate”* – as rationale for DDL’s two-model setup. This concept has appeared in various AI contexts (e.g. using an LLM to propose answers and another to check them, which is a technique seen in alignment and prompt engineering discussions before). So the generator/critic loop is **not new** on its own.

* *Unsupervised *“dreaming”* to improve knowledge:* The idea of a model churning out internal simulations or pseudo-experiences and learning from them also has precedents. Hinton’s classic *wake-sleep algorithm* (1995) had a “sleep” phase where the model *dreamed* (generated data from its model) to help train itself. In reinforcement learning, *self-play* is a related concept – e.g. AlphaGo learning by playing millions of games against itself, effectively generating its own experience data. More directly akin to Gwern’s idea, some recent LLM research has explored *self-refinement loops* (having an LLM generate a draft answer or hypothesis and then critique it, iteratively). These are typically goal-directed and not random, but they show the community has been exploring ways for models to *think in loops* without human intervention. Gwern’s DDL is unique in focusing on *serendipitous, undirected* idea generation rather than solving a given problem.

* *Memory replay and knowledge consolidation:* In neuroscience-informed AI, there’s recognition that replaying memories can yield new insights. Gwern compares DDL to how the hippocampus might replay experiences during sleep, though notes the daydreaming loop deals with *working* knowledge instead. Some AI works on agent planning use a replay of past events to plan new strategies (though again, usually directed). The general principle of *recombining learned pieces to discover new patterns* underlies many evolutionary algorithms and novelty search methods (for example, genetic algorithms randomly mutate and cross over known solutions to find new ones, then a fitness function selects the promising ones – conceptually similar to generate-and-filter).

In summary, each ingredient of the loop (random concept retrieval, combining for creative output, filtering with a critic, and learning from good outputs) **has been discussed or implemented in some form** in past work. However, **the “whole package” of DDL as a continual, automated *background* process for an LLM is novel**. We did not find an earlier paper or post that lays out this exact mechanism to give AI a default mode network. The AI2 experiment in 2023 was a partial step (random internal associations for creativity), but it was likely a one-off test rather than a persistent loop with memory updates. Gwern’s essay synthesizes ideas from neuroscience, creativity research, and ML (he cites wake-sleep and even an economics model of innovation as inspiration) into a concrete proposal. This specific *algorithmic blueprint* appears to be Gwern’s original contribution.

**Originality:** **Mixed.** The *conceptual components* of the daydreaming loop are not new – random combination for creativity, generator-critic models, and self-learning via generated data all have precedents. What is **innovative** is the integration of these into a single envisioned system for LLMs and the argument that this could yield the kind of out-of-the-blue breakthroughs humans have. In other words, the *particular idea of running an LLM in a perpetual brainstorming cycle to generate and incorporate novel knowledge* seems to be an original contribution of the article. We can consider the DDL proposal **largely novel as a package**, even if built on known principles.

### 4. **Practicality and Cost:** The “Daydreaming Tax”

**Gwern’s Claim:** Implementing DDL would be expensive and inefficient – an LLM would spend most of its time generating nonsense or trivial connections. The hit rate for truly valuable insights would be very low, meaning vast compute wasted on misses. Gwern dubs this overhead the *“daydreaming tax.”* He ballparks that an AI might need on the order of 20× its normal compute to run daydreaming in the background (i.e. maybe 20 dummy ideas tried for each useful one, analogous to a human needing lots of stray thoughts to hit a gem). Despite the cost, he argues this *waste* may be the necessary price of innovation, just as human creativity is metabolically costly and time-consuming.

**Precedents:** The idea that searching for novelty requires lots of trial-and-error (and thus compute) is intuitive and well-aligned with prior thinking. In the field of **novelty search** (Lehman & Stanley, 2011), researchers have emphasized that abandoning efficient, goal-directed search and instead exploring for novelty will inherently evaluate many useless variants – but can yield creative outcomes that directed search would miss. In human terms, R\&D organizations accept that many experiments or ideas will fail in order to find a breakthrough. So, Gwern’s “daydreaming tax” concept is essentially translating a well-known aspect of creativity (its **low yield** or high cost) into the LLM context. Others have made similar points: for example, some commentators noted that if an AI were to genuinely explore new ideas, it might need to run *millions* of divergent thought simulations, most going nowhere. In online discussions of AI creativity, skepticism often centers on this combinatorial explosion – *“exponential search spaces”* that render naive idea generation impractical (Z. Łukasiak’s critique of Gwern’s proposal, July 2025, highlights exactly this issue of too many combinations for too few hits).

Thus, acknowledging a high cost for open-ended creativity is **not novel** – it’s a widely understood hurdle. Gwern’s colorful term *“daydreaming tax”* is new phrasing, but the underlying idea (many random combinations will be *“useless”*) is expected. In fact, Gwern cites economist Charles Jones (2021) who showed that even as we pick the *low-hanging fruit* of ideas, there remains a vast combination space that can keep yielding innovation – implying a long tail of search. This reinforces that the inefficiency is known but also that it may ultimately pay off in a constant stream of innovations.

**Originality:** **Low.** The recognition that an AI default-mode search would be computationally expensive with a low success rate is not a novel insight; it mirrors established ideas about the nature of creative search. Gwern’s contribution here is mostly in quantifying it and framing it as a necessary **trade-off**: *“LLMs may need to become slow & expensive so they can be fast & cheap”* for end-users later. The insight that *creativity isn’t free* is well-known, though Gwern applies it pointedly to LLM economics.

### 5. **Implications:** Data Moats and Next-Generation Training Data

**Gwern’s Claim:** Paradoxically, dedicating expensive compute to an AI’s daydreams could make *future* AI systems more efficient for users. The essay suggests a scenario where companies run *“expensive, daydreaming AIs”* to generate a trove of *proprietary training data* – novel insights, hypotheses, designs, etc. that did not previously exist. This unique data can then be used to train smaller, cheaper models that have those insights baked in, thereby *“offering a path around the looming data wall.”* In effect, Gwern argues that daydreaming AIs could solve the problem of limited human-generated data by autonomously creating new knowledge. Moreover, these novel discoveries would act as a **moat** against competitors: a rival can’t simply distill or copy your model’s capabilities if those capabilities stem from secret insights the model discovered and no one else knows to ask for.

**Precedents:** There are a few threads to unpack here:

* *Using AI to generate training data:* This idea is already being practiced in a simpler form. For example, *self-instruct* techniques were used in 2022–23 to create instruction-following datasets: OpenAI’s and Stanford’s teams had GPT-3 produce thousands of Q\&A pairs or instructions, which were then used to fine-tune smaller models (e.g. the Stanford **Alpaca** model was trained on 52,000 instructions generated by GPT-3.5). This showed that AI can **bootstrap** additional training data beyond what humans wrote. However, these synthetic data were largely *in-distribution* (i.e. mimicking human-style queries and answers) – they did not constitute fundamentally new scientific insights or knowledge leaps. Gwern’s vision is a step further: generating data that is not just more of the same, but *original breakthroughs*. Still, the general notion of *AI-sourced training data* predates the essay and is increasingly common (instruct datasets, roleplay transcripts, etc.).

* *Avoiding the “data wall”:* The community has been aware that we might exhaust high-quality human text data for training future models. One oft-discussed solution is exactly to use AI to **synthesize new data**. For instance, AI safety and policy forums (LessWrong/Alignment Forum) have discussed that large models could eventually produce most of their own training corpus. In one Alignment Forum comment (2023), it was noted that *“to avoid running out of data, we might have AI generate proprietary training data for the next generation of models”* – essentially the same idea Gwern articulates, indicating it was floating around among researchers. Gwern even links to a LessWrong post about creating proprietary data. So while his essay popularized the concept, it was *not the first* to suggest that a *data moat* might come from AI-generated content.

* *Moat against distillation:* The specific point that these AI-derived insights form a protective moat (because others can’t query your model for answers they don’t know about) is a clever strategic insight. This angle – *models that invent proprietary knowledge are harder to copy* – was not widely discussed prior to 2025. It builds on the known phenomenon that open-sourcing model weights or training on public data makes it easy to replicate capabilities, whereas unique data is a last refuge of competitive advantage. Some analysts had mentioned that the **only** long-term moat in LLMs might be access to data that others don’t have (for example, discussions around OpenAI’s competitive edge often cited its private user interaction data or fine-tuning data). Gwern’s suggestion that *self-discovered* ideas could be such data is relatively novel. It extends the concept of “AI finds new knowledge” into an economic argument for closed-source efforts.

In summary, using AI daydreaming to generate novel training data is a logical extrapolation of existing trends (self-play in games, synthetic data for fine-tuning, etc.), but framing it as a solution to the data scarcity problem and a moat is a **fresh perspective**. We did not find any earlier publication that put these pieces together in the same way, though elements were discussed separately (synthetic data generation, data as moats, etc.). The closest precedents are forum speculations and internal discussions in 2023–24 about needing “data generation” as we hit the end of the internet text.

**Originality:** **Moderate.** The building blocks of this idea existed (AI-generated training data was already a practice), but Gwern’s combined vision – a *tiered AI ecosystem* where expensive dreamer models feed proprietary insights into cheaper deployed models – appears original. It is a novel strategic implication drawn from the daydreaming concept. This idea was **not extensively documented in prior research literature**, aside from passing mentions on blogs/forums, making it a notable contribution of the essay.

## Summary of Each Idea’s Originality

In the table below, we compile the main ideas from *“AI Daydreaming”*, note earlier sources or precedents (especially from AI research and communities *before mid-2025*), and assess the novelty of each idea:

| **Idea from *“AI Daydreaming”***                                                                              | **Prior Discussion/Precedents**                                                                                                                                                                                                                                                                                                                                                                                                                                                  | **Novelty Assessment**                                                                                                                                                                                                                                                                         |
| ------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **LLMs lack continual learning** (no online updating of knowledge)                                            | Widely recognized in ML; e.g. *dynamic evaluation* for LMs known since 2018. AI commentators in 2023–25 (Patel, etc.) already lamented that *“LLMs don’t get better over time”* without retraining.                                                                                                                                                                                                                                                                              | **Not novel.** This limitation is well-known and frequently discussed as a major hurdle. Gwern is reiterating an established point.                                                                                                                                                            |
| **LLMs lack a “default mode network”** (no spontaneous or background thought)                                 | Conceptual parallels drawn by others: e.g. Medium/LinkedIn blogs in 2023–24 noted AI’s absence of mind-wandering and continuous internal thought. Early research explored simulated *mind-wandering* in 2023 (AI2 paper) with some success. AI forums had begun using the DMN analogy to describe this gap in LLMs.                                                                                                                                                              | **Partly novel.** The idea that AI might need a default mode was **emerging** in discourse. Gwern’s framing is timely but not the first; it refines an idea already “in the air.”                                                                                                              |
| **Day-Dreaming Loop (DDL)** – continuous random concept recombination + generator–critic + learning from hits | Elements have clear precedents: *combinatorial creativity* (Koestler’s bisociation, 1960s); using LLMs to mix random concepts for ideas (e.g. “trisociation” in 2023); generator-and-filter architectures (adversarial or two-step approaches known in ML); self-generated training data (wake-sleep, self-play in games, etc.). *No prior work* packaged these into a single always-on algorithm for LLMs, though similar ideas were hinted (e.g. AllenAI’s experiment, above). | **Moderately novel.** Most components are established, but the **integrated concept** of an LLM running its own perpetual creative brainstorming loop is new. Gwern’s DDL proposal appears to be an original synthesis filling a gap that hadn’t been concretely addressed in literature.      |
| **“Daydreaming tax”** – significant compute cost for low hit-rate creative search                             | Understood in creativity and exploration research. High waste is expected in open-ended search (acknowledged in evolutionary algorithms and human creativity studies). Critics of AI creativity already noted the impracticality of brute-force idea generation due to combinatorial explosion. Gwern’s 20:1 estimate is a rough intuition, not from prior data, but the concept of costly search is not unique.                                                                 | **Not novel.** It’s an accepted fact that novelty-generation is resource-intensive. The term is new, but the insight is conventional. Gwern reinforces it well, but does not introduce it.                                                                                                     |
| **Using daydreaming AIs to produce proprietary training data (“data moat”)**                                  | **Partially foreshadowed:** Synthetic data generation for model training is already in use (e.g. GPT-3 → Alpaca dataset in 2023). The idea of AI-created data as a competitive advantage was mentioned on forums (LessWrong) prior to 2025, though not fleshed out in published research. No evidence of a formal paper on “data moat via AI ideas” before.                                                                                                                      | **Relatively novel.** While building on known practices (self-generated data), the specific strategic vision – running expensive “dreamers” to leap the data-scarcity gap and outpace competitors – is a new contribution of the essay. Few had explicitly made this argument in depth before. |

**Proportion of New vs. Established Ideas:** In rough terms, about **20–30%** of the ideas in *“AI Daydreaming”* can be considered novel contributions, while **70–80%** draw on previously discussed concepts. The *most original* aspects are the concrete proposal of the daydreaming loop itself and its strategic application to creating a data advantage – these give the essay a unique “whole package.” The other foundations (continuous learning, the importance of mind-wandering, the cost of creativity) were already well known in the AI research and cognitive science communities. In sum, Gwern has combined several existing threads into a compelling narrative and blueprint. The novelty lies less in each individual insight and more in the **synthesis and emphasis**: he identified a puzzle (LLMs lack breakthroughs), invoked known human analogies (learning and daydreaming), and proposed a solution that, as a whole, had not been articulated before.

## Conclusion

Gwern’s *“AI Daydreaming”* is an insightful synthesis that feels *fresh* not because each idea was unheard of, but because it weaves them into a cohesive proposal at just the right time. Many of its building blocks – lifelong learning for AIs, the value of mind-wandering, generator-and-critic creativity loops, and synthetic data generation – were circulating in AI research circles and blogs in previous years. The article’s novelty is primarily in the **whole package**: identifying the lack of a default-mode process as a key missing piece in current AI, and suggesting a practical (if expensive) way to implement one. By doing so, it highlighted a path that few had concretely charted out. In evaluation, most individual claims in the essay have clear precedents (hence are not Gwern’s unique invention), but the particular combination and the forward-looking vision (e.g. *“a future where slow, daydreaming AIs make the next generation of models smarter”*) represent an original contribution to the conversation.

Ultimately, *“AI Daydreaming”* is less about an unprecedented discovery and more about crystallizing a set of ideas into a compelling hypothesis. It advanced the discussion on AI creativity by pointing out that achieving true innovation might require embracing qualities long deemed “wasteful” in machines – continuous learning, aimless exploration, and internal **daydreams**. That reframing, backed by both old and new thinking, is what gives the essay its impact. The majority of its components were already known, but Gwern’s synthesis is novel enough to prompt new experimentation and debate on enabling AI to *dream up* the breakthroughs of tomorrow.

**Sources:** The analysis above draws on Gwern’s original essay and compares it with prior work and commentary from AI researchers and bloggers, including Dwarkesh Patel, Irina Taleska (Solveo), academic literature on AI creativity and learning, and discussions on forums like LessWrong. These references illustrate which ideas were established and which were first synthesized in *“AI Daydreaming.”*
