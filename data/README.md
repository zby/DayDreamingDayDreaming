# Data Directory Structure

This document explains the data directory structure for the DayDreaming Dagster Pipeline project.

## Directory Overview

The data directory follows a numbered pipeline structure that represents the flow of data through the system:

```
data/
├── 1_raw/          # Input data (tracked in git)
├── cohorts/        # Cohort manifests and membership (tracked in git)
├── 3_generation/   # LLM generation outputs (not tracked)
├── 4_evaluation/   # LLM evaluation outputs (not tracked)  
├── 5_parsing/      # Parsed evaluation results (not tracked)
├── cohorts/        # Cohort manifolds, memberships, and cohort-scoped reports
└── 7_reporting/    # Error logs and reports (not tracked)
```

## Directory Details

### 1_raw/ (Input Data - Tracked)
**Purpose**: Source data that defines the experiment parameters

**Contents**:
- `concepts/` - Concept definitions and descriptions at different levels (sentence, paragraph, article)
  - `concepts_metadata.csv` - Concept metadata with active/inactive flags
  - `descriptions-*/` - Concept description files organized by detail level
- `*_templates.csv` - Template metadata with active/inactive flags
- `*_templates/` - Jinja2 prompt template files
- `llm_models.csv` - Available LLM models with generation/evaluation flags

**Tracking**: ✅ Tracked in git (these are the experiment definitions)

### cohorts/ (Cohorts & Membership - Tracked)  
**Purpose**: Authoritative cohort manifest and normalized membership; registers dynamic partitions

**Contents**:
- `<cohort_id>/manifest.json` — deterministic manifest (combos/templates/models)
- `<cohort_id>/membership.csv` — normalized rows with parent links:
  - Columns: `stage, gen_id, cohort_id, parent_gen_id, combo_id, template_id, llm_model_id`

**Tracking**: ✅ Tracked in git (reproducible cohort definitions)

**Generated by**: `cohort_id`, `content_combinations`, and `cohort_membership` assets

### 3_generation/ (LLM Generation - Not Tracked)
**Purpose**: LLM-generated responses to concept combination prompts  

**Contents**:
- `generation_prompts/` - Generated prompts (one file per partition)
- `generation_responses/` - LLM responses to prompts (one file per partition)

**Tracking**: ❌ Not tracked (large, regenerable, API-dependent)

**Generated by**: `group:generation_draft` and `group:generation_essays` assets (legacy `generation_links` supported)

### 4_evaluation/ (LLM Evaluation - Not Tracked)
**Purpose**: LLM-based evaluation of generation responses

**Contents**:
- `evaluation_prompts/` - Generated evaluation prompts (one file per partition)  
- `evaluation_responses/` - LLM evaluation responses (one file per partition)

**Tracking**: ❌ Not tracked (large, regenerable, API-dependent)

**Generated by**: `group:evaluation` assets

### 5_parsing/ (Parsed Results - Not Tracked)
**Purpose**: Structured data extracted from LLM evaluation responses

**Contents**:
- `cohort_aggregated_scores.csv` - Parsed evaluation rows scoped to the active cohort
- `parsed_scores.csv` - Historical name retained in some backfills; regenerable from cohort outputs
- Other parsed result files as needed

**Tracking**: ❌ Not tracked (derived from 4_evaluation/)

**Generated by**: `group:results_processing` assets (e.g., `cohort_aggregated_scores`)

### cohorts/<cohort_id>/reports/ (Cohort Reports - Not Tracked)
**Purpose**: Cohort-scoped parsing, summary, and analysis outputs

**Contents**:
- `parsing/aggregated_scores.csv` - Evaluation rows scoped to the cohort
- `summary/generation_scores.csv` - Evaluation pivots by generation response
- `summary/final_results.csv` - Aggregated statistics per analysis axis
- `summary/perfect_score_paths.csv` - Responses with perfect scores
- `analysis/variance_analysis.csv` - Variance breakdown across evaluators/templates
- Additional cohort reports generated as needed

**Tracking**: ❌ Not tracked (derived from gens store + membership)

**Generated by**: `group:results_processing` and `group:results_summary` assets

### 7_cross_experiment/ (Shared Tables - Not Tracked)
**Purpose**: Rebuilt views that span cohorts/experiments

**Contents**:
- `aggregated_scores.csv` - Cross-experiment aggregation rebuilt from `data/gens/**`
- `evaluation_scores_by_template_model.csv` and other pivots from rebuild scripts

**Tracking**: ❌ Not tracked (rebuilt on demand)

**Generated by**: Scripts (`aggregate_scores.py`, `build_pivot_tables.py`) and cross-experiment assets

### 7_reporting/ (Error Logs - Not Tracked)
**Purpose**: Error logs and diagnostic information

**Contents**:
- Error logs from failed LLM calls
- Diagnostic reports
- Processing statistics

**Tracking**: ❌ Not tracked (operational data)

**Generated by**: Various assets for error reporting

## Data Flow

```
1_raw → cohort_membership → gens/<stage> → cohorts/<id>/reports
   ↓                                                     ↓
   └─ Experiment Definition                         Final Results ─┘
                             ↓
                             └─ 7_cross_experiment (rebuilt views)
```

## Tracking Strategy

**What's Tracked**:
- `1_raw/` - Experiment definitions (concepts, templates, models)
- `cohorts/<cohort_id>/` - Cohort manifest and membership

**What's Not Tracked**:  
- `cohorts/<cohort_id>/reports/`, `3_generation/` through `7_cross_experiment/`, and `7_reporting/` - Large, regenerable, or operational data
- See `.gitignore` for complete list

**Rationale**: The tracked data allows complete reproduction of experiments, while the untracked data can be regenerated from the tracked definitions.

## Usage Notes

- Raw and cohort assets auto-update when `data/1_raw/**/*` changes (daemon required). Start Dagster with the daemon to keep them fresh.
- Seed: materialize `cohort_id,cohort_membership` once to register partitions for the active cohort.
- The pipeline creates directories automatically as needed
- Use Dagster UI to monitor data generation progress
- Large datasets in generation/evaluation folders can be safely deleted and regenerated

## Data Retention and Overwrite Behavior

This project intentionally separates tracked, reproducible definitions from untracked, regenerable artifacts.

- **Tracked (retained across materializations)**:
- `1_raw/` CSVs and `cohorts/<cohort_id>/membership.csv` are overwritten in-place by their assets to reflect the latest selection logic, but are version-controlled in git. You can revert if needed.
  - `data/combo_mappings.csv` is append-only and is never cleared automatically. It preserves stable `combo_id` mappings for cross-experiment reproducibility.

- **Untracked (safe to delete and regenerate)**:
- `cohorts/<cohort_id>/reports/` cohort outputs
- `3_generation/` prompts and responses
- `4_evaluation/` prompts and responses
- `5_parsing/` parsed staging files (legacy)
- `7_reporting/` error logs

### What gets deleted or reset when you (re)materialize

- Dynamic partitions for draft/essay/evaluation are (re)registered by `cohort_membership`. Existing cohort partitions are pruned cohort‑scoped before re‑registering.
- CSV outputs (membership) are rewritten by their producing assets.
- Gens-store artifacts under `data/gens/<stage>/<gen_id>/` are not deleted by the pipeline. Existing files remain on disk unless you remove them manually.

### Overwrite guards for LLM artifacts

- Prompts (`data/gens/<stage>/<gen_id>/prompt.txt`) are configured to allow overwrite so they reflect the current template code.
- Responses (`data/gens/<stage>/<gen_id>/parsed.txt`) are protected against overwrite by default. If a response file already exists for a partition key, the run will fail with a clear error instead of silently overwriting. To regenerate, either:
  - delete the specific file(s), or
  - change the partition key (e.g., different template/model), or
  - intentionally configure the IO manager with `overwrite=True` (not recommended for responses).

### Stable partition keys and filenames

- Filenames are derived from partition keys. Generation partitions follow `{combo_id}_{template_id}_{model_id}` and evaluation partitions append evaluation identifiers. With stable combo IDs (`combo_v1_<12-hex>`), filenames are deterministic and do not collide across runs unless the same task is intentionally repeated.

### Full reset (keeping raw data)

To start fresh while retaining experiment definitions, you can remove generated artifacts:

```bash
rm -rf data/cohorts/*
rm -rf data/gens/*
rm -rf data/5_parsing/*
rm -rf data/7_reporting/*
```

Recreate cohort membership and downstream data by materializing (seed once; daemon handles updates):

```bash
uv run dagster asset materialize --select "cohort_id,cohort_membership,content_combinations" -f src/daydreaming_dagster/definitions.py
```
