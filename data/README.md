# Data Directory Structure

This document explains the data directory structure for the DayDreaming Dagster Pipeline project.

## Directory Overview

The data directory follows a numbered pipeline structure that represents the flow of data through the system:

```
data/
├── 1_raw/          # Input data (tracked in git)
├── 2_tasks/        # Generated task definitions (tracked in git)
├── 3_generation/   # LLM generation outputs (not tracked)
├── 4_evaluation/   # LLM evaluation outputs (not tracked)  
├── 5_parsing/      # Parsed evaluation results (not tracked)
├── 6_summary/      # Summary and analysis results (not tracked)
└── 7_reporting/    # Error logs and reports (not tracked)
```

## Directory Details

### 1_raw/ (Input Data - Tracked)
**Purpose**: Source data that defines the experiment parameters

**Contents**:
- `concepts/` - Concept definitions and descriptions at different levels (sentence, paragraph, article)
  - `concepts_metadata.csv` - Concept metadata with active/inactive flags
  - `descriptions-*/` - Concept description files organized by detail level
- `*_templates.csv` - Template metadata with active/inactive flags
- `*_templates/` - Jinja2 prompt template files
- `llm_models.csv` - Available LLM models with generation/evaluation flags

**Tracking**: ✅ Tracked in git (these are the experiment definitions)

### 2_tasks/ (Task Definitions - Tracked)  
**Purpose**: Generated task combinations and partition definitions

**Contents**:
- `content_combinations_csv.csv` - Normalized table of concept combinations
- `generation_tasks.csv` - LLM generation task definitions with partition keys
- `evaluation_tasks.csv` - LLM evaluation task definitions with partition keys

**Tracking**: ✅ Tracked in git (reproducible task definitions)

**Generated by**: `group:llm_tasks` assets

### 3_generation/ (LLM Generation - Not Tracked)
**Purpose**: LLM-generated responses to concept combination prompts  

**Contents**:
- `generation_prompts/` - Generated prompts (one file per partition)
- `generation_responses/` - LLM responses to prompts (one file per partition)

**Tracking**: ❌ Not tracked (large, regenerable, API-dependent)

**Generated by**: `group:llm_generation` assets

### 4_evaluation/ (LLM Evaluation - Not Tracked)
**Purpose**: LLM-based evaluation of generation responses

**Contents**:
- `evaluation_prompts/` - Generated evaluation prompts (one file per partition)  
- `evaluation_responses/` - LLM evaluation responses (one file per partition)

**Tracking**: ❌ Not tracked (large, regenerable, API-dependent)

**Generated by**: `group:llm_evaluation` assets

### 5_parsing/ (Parsed Results - Not Tracked)
**Purpose**: Structured data extracted from LLM evaluation responses

**Contents**:
- `parsed_scores.csv` - Extracted scores and metadata from evaluation responses
- Other parsed result files as needed

**Tracking**: ❌ Not tracked (derived from 4_evaluation/)

**Generated by**: `group:results_processing` assets

### 6_summary/ (Analysis Results - Not Tracked)  
**Purpose**: Final analysis and summary of experiment results

**Contents**:
- `final_results.csv` - Aggregated experiment results
- `perfect_score_paths.csv` - Analysis of perfect scoring combinations
- Other summary and analysis files

**Tracking**: ❌ Not tracked (derived from 5_parsing/)

**Generated by**: `group:results_processing` assets

### 7_reporting/ (Error Logs - Not Tracked)
**Purpose**: Error logs and diagnostic information

**Contents**:
- Error logs from failed LLM calls
- Diagnostic reports
- Processing statistics

**Tracking**: ❌ Not tracked (operational data)

**Generated by**: Various assets for error reporting

## Data Flow

```
1_raw → 2_tasks → 3_generation → 4_evaluation → 5_parsing → 6_summary
   ↓                                                           ↓
   └─ Experiment Definition                              Final Results ─┘
```

## Tracking Strategy

**What's Tracked**:
- `1_raw/` - Experiment definitions (concepts, templates, models)
- `2_tasks/` - Reproducible task definitions

**What's Not Tracked**:  
- `3_generation/` through `7_reporting/` - Large, regenerable, or operational data
- See `.gitignore` for complete list

**Rationale**: The tracked data allows complete reproduction of experiments, while the untracked data can be regenerated from the tracked definitions.

## Usage Notes

- Always materialize `group:raw_data,group:llm_tasks` before running LLM generation
- The pipeline creates directories automatically as needed
- Use Dagster UI to monitor data generation progress
- Large datasets in generation/evaluation folders can be safely deleted and regenerated