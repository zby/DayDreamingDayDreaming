# Data Directory Structure

This document explains the data directory structure for the DayDreaming Dagster Pipeline project.

## Directory Overview

The data directory follows a numbered pipeline structure that represents the flow of data through the system:

```
data/
├── 1_raw/          # Input data (tracked in git)
├── 2_tasks/        # Generated task definitions (tracked in git)
├── 3_generation/   # LLM generation outputs (not tracked)
├── 4_evaluation/   # LLM evaluation outputs (not tracked)  
├── 5_parsing/      # Parsed evaluation results (not tracked)
├── 6_summary/      # Summary and analysis results (not tracked)
└── 7_reporting/    # Error logs and reports (not tracked)
```

## Directory Details

### 1_raw/ (Input Data - Tracked)
**Purpose**: Source data that defines the experiment parameters

**Contents**:
- `concepts/` - Concept definitions and descriptions at different levels (sentence, paragraph, article)
  - `concepts_metadata.csv` - Concept metadata with active/inactive flags
  - `descriptions-*/` - Concept description files organized by detail level
- `*_templates.csv` - Template metadata with active/inactive flags
- `*_templates/` - Jinja2 prompt template files
- `llm_models.csv` - Available LLM models with generation/evaluation flags

**Tracking**: ✅ Tracked in git (these are the experiment definitions)

### 2_tasks/ (Task Definitions - Tracked)  
**Purpose**: Generated task combinations and partition definitions

**Contents**:
- `draft_generation_tasks.csv` - Phase‑1 (draft) generation task definitions with partition keys
- `essay_generation_tasks.csv` - Phase‑2 (essay) generation task definitions with partition keys
- `evaluation_tasks.csv` - LLM evaluation task definitions with partition keys
- `selected_combo_mappings.csv` - Curated subset of combinations (row‑subset of `data/combo_mappings.csv`)

**Tracking**: ✅ Tracked in git (reproducible task definitions)

**Generated by**: `group:task_definitions` assets and curation scripts (for selected combos)

### 3_generation/ (LLM Generation - Not Tracked)
**Purpose**: LLM-generated responses to concept combination prompts  

**Contents**:
- `generation_prompts/` - Generated prompts (one file per partition)
- `generation_responses/` - LLM responses to prompts (one file per partition)

**Tracking**: ❌ Not tracked (large, regenerable, API-dependent)

**Generated by**: `group:generation_draft` and `group:generation_essays` assets (legacy `generation_links` supported)

### 4_evaluation/ (LLM Evaluation - Not Tracked)
**Purpose**: LLM-based evaluation of generation responses

**Contents**:
- `evaluation_prompts/` - Generated evaluation prompts (one file per partition)  
- `evaluation_responses/` - LLM evaluation responses (one file per partition)

**Tracking**: ❌ Not tracked (large, regenerable, API-dependent)

**Generated by**: `group:evaluation` assets

### 5_parsing/ (Parsed Results - Not Tracked)
**Purpose**: Structured data extracted from LLM evaluation responses

**Contents**:
- `parsed_scores.csv` - Extracted scores and metadata from evaluation responses
- Other parsed result files as needed

**Tracking**: ❌ Not tracked (derived from 4_evaluation/)

**Generated by**: `group:results_processing` assets

### 6_summary/ (Analysis Results - Not Tracked)  
**Purpose**: Final analysis and summary of experiment results

**Contents**:
- `final_results.csv` - Aggregated experiment results
- `perfect_score_paths.csv` - Analysis of perfect scoring combinations
- Other summary and analysis files

**Tracking**: ❌ Not tracked (derived from 5_parsing/)

**Generated by**: `group:results_processing` assets

### 7_reporting/ (Error Logs - Not Tracked)
**Purpose**: Error logs and diagnostic information

**Contents**:
- Error logs from failed LLM calls
- Diagnostic reports
- Processing statistics

**Tracking**: ❌ Not tracked (operational data)

**Generated by**: Various assets for error reporting

## Data Flow

```
1_raw → 2_tasks → 3_generation → 4_evaluation → 5_parsing → 6_summary
   ↓                                                           ↓
   └─ Experiment Definition                              Final Results ─┘
```

## Tracking Strategy

**What's Tracked**:
- `1_raw/` - Experiment definitions (concepts, templates, models)
- `2_tasks/` - Reproducible task definitions

**What's Not Tracked**:  
- `3_generation/` through `7_reporting/` - Large, regenerable, or operational data
- See `.gitignore` for complete list

**Rationale**: The tracked data allows complete reproduction of experiments, while the untracked data can be regenerated from the tracked definitions.

## Usage Notes

- Raw and task-definition assets auto-update when `data/1_raw/**/*` changes (daemon required). Start Dagster with the daemon to keep them fresh.
- Optional: seed `group:task_definitions` once to create initial CSVs and partitions.
- The pipeline creates directories automatically as needed
- Use Dagster UI to monitor data generation progress
- Large datasets in generation/evaluation folders can be safely deleted and regenerated

## Data Retention and Overwrite Behavior

This project intentionally separates tracked, reproducible definitions from untracked, regenerable artifacts.

- **Tracked (retained across materializations)**:
  - `1_raw/` and `2_tasks/` CSVs are overwritten in-place by their assets to reflect the latest selection logic, but are version-controlled in git. You can revert if needed.
  - `data/combo_mappings.csv` is append-only and is never cleared automatically. It preserves stable `combo_id` mappings for cross-experiment reproducibility.

- **Untracked (safe to delete and regenerate)**:
  - `3_generation/` prompts and responses
  - `4_evaluation/` prompts and responses
  - `5_parsing/` parsed results
  - `6_summary/` analysis outputs
  - `7_reporting/` error logs

### What gets deleted or reset when you (re)materialize

- Dynamic partitions for `generation_tasks` and `evaluation_tasks` are cleared and recreated on every materialization of those assets. This affects Dagster's notion of which partitions exist, not your files.
- CSV outputs in `2_tasks/` are rewritten by their producing assets.
- Files under `3_generation/` and `4_evaluation/` are not deleted by the pipeline. Existing files remain on disk unless you remove them manually.

### Overwrite guards for LLM artifacts

- Prompts (`3_generation/generation_prompts`, `4_evaluation/evaluation_prompts`) are configured to allow overwrite so they reflect the current template code.
- Responses (`3_generation/generation_responses`, `4_evaluation/evaluation_responses`) are protected against overwrite by default. If a response file already exists for a partition key, the run will fail with a clear error instead of silently overwriting. To regenerate, either:
  - delete the specific file(s), or
  - change the partition key (e.g., different template/model), or
  - intentionally configure the IO manager with `overwrite=True` (not recommended for responses).

### Stable partition keys and filenames

- Filenames are derived from partition keys. Generation partitions follow `{combo_id}_{template_id}_{model_id}` and evaluation partitions append evaluation identifiers. With stable combo IDs (`combo_v1_<12-hex>`), filenames are deterministic and do not collide across runs unless the same task is intentionally repeated.

### Full reset (keeping raw data)

To start fresh while retaining experiment definitions, you can remove generated artifacts:

```bash
rm -rf data/2_tasks/*.csv
rm -rf data/3_generation/*
rm -rf data/4_evaluation/*
rm -rf data/5_parsing/*
rm -rf data/6_summary/*
rm -rf data/7_reporting/*
```

Recreate tasks and downstream data by materializing (seed once; daemon handles updates):

```bash
uv run dagster asset materialize --select "group:task_definitions" -f daydreaming_dagster/definitions.py
```
