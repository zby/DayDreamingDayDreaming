A large language model (LLM) is a probabilistic sequence model trained to continue text given prior context. In practice it is accessed via two layers: (1) a core continuation engine that generates tokens conditioned on prompts and internal state, and (2) a higher-level chat interface that manages turns, roles, and system instructions. LLMs are frequently paired with complementary capabilities such as tool calling (invoking external functions or APIs), retrieval (bringing in reference materials), and short- or long-term memory buffers. This makes them suitable as a general coordination and synthesis component that can read inputs, propose structured outputs, call tools as needed, and refine results across steps under guidance.
